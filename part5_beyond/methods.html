
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Methods for Self-Supervised Learning &#8212; Music Classification: Beyond Supervised Learning, Towards Real-world Applications</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="PyTorch Tutorial" href="self-supervised-learning.html" />
    <link rel="prev" title="Introduction" href="introduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/android-chrome-512x512.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Music Classification: Beyond Supervised Learning, Towards Real-world Applications</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p class="caption" role="heading">
 <span class="caption-text">
  The Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part1_intro/what-is-music-classification.html">
   What is Music Classification?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part2_basics/input-representations.html">
   Input Representations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part2_basics/dataset.html">
   Datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part2_basics/problem-formulation.html">
   Problem Formulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part2_basics/evaluation.html">
   Evaluation
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Supervised Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part3_supervised/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part3_supervised/architectures.html">
   Architectures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part3_supervised/data-augmentation.html">
   Audio Data Augmentations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part3_supervised/tutorial.html">
   PyTorch tutorial
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Semi-supervised Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part4_beyond/introduction.html">
   Beyond Supervision
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part4_beyond/transfer-learning.html">
   Transfer Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part4_beyond/semi-supervised-learning.html">
   Semi-Supervised Learning
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Self-supervised Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Methods for Self-Supervised Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="self-supervised-learning.html">
   PyTorch Tutorial
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Towards Real-world Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part7_towards/mlops.html">
   MLOps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part7_towards/underexplored-problems.html">
   Under-Explored Problems in Academia
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Conclusion
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part8_conclusion/_intro.html">
   Conclusion
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../references.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/part5_beyond/methods.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/music-classification/tutorial"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#contrastive-learning">
   Contrastive Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#contrastive-predictive-coding">
   Contrastive Predictive Coding
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#momentum-contrast-moco">
   Momentum Contrast (MoCO)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simclr">
   SimCLR
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#contrastive-losses">
   Contrastive Losses
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pase">
   PASE
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#more-papers-on-self-supervised-learning">
   More papers on self-supervised learning
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="methods-for-self-supervised-learning">
<h1>Methods for Self-Supervised Learning<a class="headerlink" href="#methods-for-self-supervised-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="contrastive-learning">
<h2>Contrastive Learning<a class="headerlink" href="#contrastive-learning" title="Permalink to this headline">¶</a></h2>
<p>Contrastive learning is a method that describes learning representations by modeling similarity from natural variations of data.
It is often presented in the following stages:</p>
<ol class="simple">
<li><p>Encode different “views” from natural variations of a single example.</p></li>
<li><p>Train a model with metric learning using the representations from the encoder(s).</p></li>
<li><p>Use the representation of the encoder by applying another classifier for the downstream task. For evaluation, linear regression is used usually so that we can focus on the performance of the pre-trained encoder rather than the that of the added classifier.</p></li>
</ol>
</div>
<div class="section" id="contrastive-predictive-coding">
<h2>Contrastive Predictive Coding<a class="headerlink" href="#contrastive-predictive-coding" title="Permalink to this headline">¶</a></h2>
<p>Contrastive predictive coding (CPC) was introduced by Aäron van den Oord et al. (2018) <span id="id1">[<a class="reference internal" href="../references.html#id69">OLV18</a>]</span> as a universal framework of representation learning. The data can be an image, in which neighbouring <em>patches</em> usually share spatial information locally. In the case of speech signals, it could be the phonemes that should be similar with the neighbors. Conversely, on a more global level, we expect a different pattern. For example, the chorus of a song is expected to repeat in another part of our audio signal.</p>
<p>In CPC, these related observations are mapped similarly as a representation in a latent space. Their hypothesis is that predictions of related observations are often conditionally dependent on similar, high-level pieces of latent information.</p>
<p>To test their hypothesis, they propose the following:</p>
<ol class="simple">
<li><p>First, complex natural data, such as images and audio, are compressed into a latent embedding space. This makes it easier to model the predictions of related observations.</p></li>
<li><p>A more expressive <em>(read: larger, more powerful)</em> autoregressive model uses the representations in this latent space to make predictions for future observations. These observations are mapped to the corresponding representation.</p></li>
<li><p>The InfoNCE loss uses a cross-entropy loss to quantify how well the model can classify these future representations from a set of unrelated, <em>“negative”</em> examples [CITATION]. This loss is inspired by Noise Contrastive Estimation [CITATION].</p></li>
</ol>
<p><img alt="Contrastive Predictive Coding applied to audio. (From: van den Oord, et al. (2018))" src="../_images/cpc_audio.png" /></p>
<div class="tip admonition">
<p class="admonition-title">Mini-batch composition</p>
<p>In CPC, a mini-batch is made of $N$ examples that are chosen randomly from the full training dataset. Within this batch, a single positive example (also known as the <em>anchor</em> sample) and $N-1$ negative examples are used to compute the cross-entropy loss for classifying the correct positive example. In this way, the internal structure of the data is leveraged to obtain a loss signal that we can backpropagate.</p>
</div>
</div>
<div class="section" id="momentum-contrast-moco">
<h2>Momentum Contrast (MoCO)<a class="headerlink" href="#momentum-contrast-moco" title="Permalink to this headline">¶</a></h2>
<p>Momentum Contrast was proposed in <span id="id2">[<a class="reference internal" href="../references.html#id72">HFW+20</a>]</span>. It was one of the first papers to close the gap between unsupervised and supervised learning approaches on some vision tasks. In MoCO, a dictionary of examples in the data are maintained as a queue. Each example in the mini-batch is encoded, and put in front of the queue, while the last item in the dictionary is subsequently dequeued. The pretext task used in MoCO is to define a contrastive loss on the query and the keys of the dictionary: a query matches the key when the query is an embedding of a different <em>view</em> of the same datapoint. For example: if the query is an embedding of the bassoon solo in Stravinsky’s “Rite of Spring”, it should match with the key that corresponds to the “Rite of Spring”. The encoded query should be similar to its corresponding key, and dissimilar to other keys in the dictionary.</p>
<p>Training a Momentrum Contrast encoder is done with positive and negative pairs of examples in a mini-batch. The positive example pairs are made of queries that correspond to keys of the current mini-batch. The negative pairs are queries of the current mini-batch and keys from past mini-batches.</p>
<a class="reference internal image-reference" href="../_images/momentum_contrast.png"><img alt="Momentum Contrast training protocol (From Momentum Contrast for Unsupervised Visual Representation Learning (2019)" class="align-center" src="../_images/momentum_contrast.png" style="width: 400px;" /></a>
<p>The keys are encoded by a <em>“slowly progressing”</em> encoder, because the dictionary’s keys are drawn over multiple mini-batches. This encoder is implemented as a momentum-based moving average. We therefore have two encoders: an encoder for the queries and a momentum-encoder for the keys. The main difference between these two encoders is in the way they are updated. The query encoder is updated by backpropagation while the momentum encoder is updated by a linear interpolation of the query and the momentum encoder.</p>
<div class="tip admonition">
<p class="admonition-title">Tip</p>
<p>An advantage of Momentum Contrast is that the batch size is not related to the number of negative examples candidates in the dictionary lookup. Even for smaller batch sizes, the performance of Momentum Contrast is consistent. This can be especially useful for (raw) audio, for which it is often harder to compute with larger mini-batch sizes due to GPU memory constraints.</p>
</div>
<p><img alt="Momentum Contrast vs. other contrastive loss mechanisms (Source: Momentum Contrast for Unsupervised Visual Representation Learning (2019))" src="../_images/momentum_contrast_compared.png" /></p>
</div>
<div class="section" id="simclr">
<h2>SimCLR<a class="headerlink" href="#simclr" title="Permalink to this headline">¶</a></h2>
<p>SimCLR was introduced in <span id="id3">[<a class="reference internal" href="../references.html#id75">CKNH20</a>]</span> as a simple contrastive learning approach to learn strong visual representations. It leverages strong image data augmentations, large batch sizes, a single large encoder and a simple contrastive loss to pre-train an encoder that learns effective representations. These representations are used to train very effective linear classifiers in various downstream image classification tasks.</p>
<p>For each image example in the mini-batch, two augmented (but correlated!) views are taken. This is done by a series of data augmentations that are applied randomly to each example. This will naturally yield $2N$ datapoints per mini-batch. Each of these augmented views are then embedded using a standard ResNet encoder network. While these representations are used during linear evaluation, during the pre-training stage these representations are projected to a different latent space by a small linear layer on which the contrastive loss is computed.</p>
<p>During pre-training, the network only learns from the contrastive loss: the labels are only used during the linear evaluation phase (see the previous section for more details).</p>
<a class="reference internal image-reference" href="../_images/simclr_contrastive_learning.png"><img alt="../_images/simclr_contrastive_learning.png" class="align-center" src="../_images/simclr_contrastive_learning.png" style="width: 400px;" /></a>
</div>
<div class="section" id="contrastive-losses">
<h2>Contrastive Losses<a class="headerlink" href="#contrastive-losses" title="Permalink to this headline">¶</a></h2>
<a class="reference internal image-reference" href="https://i.imgur.com/2uZeF4U.png"><img alt="Figure 1 from &quot;Improved Baselines with Momentum Contrastive Learning&quot; (Chen et al., 2020)" class="align-center" src="https://i.imgur.com/2uZeF4U.png" style="width: 600px;" /></a>
<p>Many contrastive learning methods use a variant of a contrastive loss function. The contrastive loss function was first introduced in Noise Contrastive Estimation <span id="id4">[<a class="reference internal" href="../references.html#id70">GHyvarinen10</a>]</span> and subsequently the InfoNCE loss from Contrastive Predictive Coding <span id="id5">[<a class="reference internal" href="../references.html#id69">OLV18</a>]</span>.</p>
<p>This loss can be minimized using a variety of methods, which mostly differ in the way they keep track of the keys of data examples. In SimCLR <span id="id6">[<a class="reference internal" href="../references.html#id75">CKNH20</a>]</span>, a single batch consists both of “positive” and “negative” pairs, which act as “keys” to the original examples. These are updated end-to-end by back-propagation. To increase the complexity of the contrastive learning task, it requires a large batch size to contain more negative examples. In Momentum Contrast, the negative examples’ keys are maintained in a queue. Note that only the queries and the positive keys in a single batch are encoded.</p>
<a class="reference internal image-reference" href="../_images/equation_1.png"><img alt="../_images/equation_1.png" class="align-center" src="../_images/equation_1.png" style="width: 400px;" /></a>
</div>
<div class="section" id="pase">
<h2>PASE<a class="headerlink" href="#pase" title="Permalink to this headline">¶</a></h2>
<p>PASE was proposed in <span id="id7">[<a class="reference internal" href="../references.html#id78">PRS+19</a>]</span>. It demonstrated that useful representations for speech recognition can be learned by defining multiple pretext tasks that jointly optimize an encoder neural network. The encoder distributes the representations of the input data to multiple, small feed-forward neural networks (called <em>workers</em>) that jointly solve different pretext tasks. Each worker is composed of a single hidden layer, and either solves a regression or binary classification task. These smaller feed-forward layers are chosen because the emphasis on learning the more expressive representations is put on the larger encoder, i.e., the encoder should learn more high-level features that can be used by the <em>worker</em> networks to help solve their tasks. After pre-training the network in a self-supervised manner, the learned representations are evaluated in the task of speaker recognition, emotion recognition and phoneme recognition.</p>
<p><img alt="The PASE architecture (Image source: Learning Problem-agnostic Speech Representations from Multiple Self-supervised Tasks (2019))" src="../_images/pase_architecture.png" /></p>
<p>The improved version of PASE, which was called PASE+, uses a set of audio data augmentations to improve the robustness of the learned representations for the downstream task.</p>
</div>
<div class="section" id="more-papers-on-self-supervised-learning">
<h2>More papers on self-supervised learning<a class="headerlink" href="#more-papers-on-self-supervised-learning" title="Permalink to this headline">¶</a></h2>
<p>The following is a short list of important papers in self-supervised learning, of which a few are discussed more in-depth in this tutorial:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Paper</p></th>
<th class="head"><p>Year</p></th>
<th class="head"><p>Tasks</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://arxiv.org/abs/1807.03748">Representation Learning with Contrastive Predictive Coding</a></p></td>
<td><p>2018</p></td>
<td><p>Speech, images, text, reinforcement learning</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</a></p></td>
<td><p>2014</p></td>
<td><p>Theoretical</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://arxiv.org/abs/1505.05192">Unsupervised Visual Representation Learning by Context Prediction</a></p></td>
<td><p>2015</p></td>
<td><p>Images</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://arxiv.org/abs/1911.05722">Momentum Contrast for Unsupervised Visual Representation Learning</a></p></td>
<td><p>2019</p></td>
<td><p>Images</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://arxiv.org/abs/1904.05862">wav2vec: Unsupervised Pre-training for Speech Recognition</a></p></td>
<td><p>2019</p></td>
<td><p>Speech recognition</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://arxiv.org/abs/1904.03416">Learning Problem-agnostic Speech Representations from Multiple Self-supervised Tasks</a></p></td>
<td><p>2019</p></td>
<td><p>Speech recognition</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://arxiv.org/abs/2006.07733">Bootstrap your own latent: A new approach to self-supervised Learning</a></p></td>
<td><p>2020</p></td>
<td><p>Images</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://arxiv.org/abs/2002.05709">A Simple Framework for Contrastive Learning of Visual Representations</a></p></td>
<td><p>2020</p></td>
<td><p>Images</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://arxiv.org/abs/2010.10915">Contrastive learning of general-purpose audio representations</a></p></td>
<td><p>2020</p></td>
<td><p>Sound classification</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://arxiv.org/abs/2103.09410">Contrastive Learning of Musical Representations</a></p></td>
<td><p>2021</p></td>
<td><p>Music classification</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://arxiv.org/abs/2004.10120">Vector Quantized Contrastive Predictive Coding for Template-based Music Generation</a></p></td>
<td><p>2021</p></td>
<td><p>Music generation</p></td>
</tr>
</tbody>
</table>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "music-classification/tutorial",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./part5_beyond"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="introduction.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Introduction</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="self-supervised-learning.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">PyTorch Tutorial</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Minz Won, Janne Spijkervet, Keunwoo Choi<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-208590538-1', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>