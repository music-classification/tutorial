
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Methods for Self-Supervised Learning &#8212; Music Classification: Beyond Supervised Learning, Towards Real-world Applications</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="PyTorch Tutorial" href="self-supervised-learning.html" />
    <link rel="prev" title="Introduction" href="introduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Music Classification: Beyond Supervised Learning, Towards Real-world Applications</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  The Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part1_intro/what-is-music-classification.html">
   What is Music Classification?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part2_basics/input-representations.html">
   Input Representations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part2_basics/dataset.html">
   Datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part2_basics/problem-formulation.html">
   Problem Formulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part2_basics/evaluation.html">
   Evaluation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Supervised Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part3_supervised/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part3_supervised/architectures.html">
   Architectures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part3_supervised/data-augmentation.html">
   Audio Data Augmentations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part3_supervised/tutorial.html">
   PyTorch tutorial
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Semi-supervised Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part4_beyond/introduction.html">
   Beyond Supervision
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part4_beyond/transfer-learning.html">
   Transfer Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part4_beyond/semi-supervised-learning.html">
   Semi-Supervised Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Self-supervised Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Methods for Self-Supervised Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="self-supervised-learning.html">
   PyTorch Tutorial
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Towards Real-world Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part7_towards/mlops.html">
   MLOps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part7_towards/underexplored-problems.html">
   Under-Explored Problems in Academia
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Conclusion
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part8_conclusion/_intro.html">
   Conclusion
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../references.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/part5_beyond/methods.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/music-classification/tutorial"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#contrastive-learning">
   Contrastive Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#contrastive-predictive-coding">
   Contrastive Predictive Coding
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#momentum-contrast-moco">
   Momentum Contrast (MoCO)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simclr">
   SimCLR
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#contrastive-losses">
   Contrastive Losses
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pase">
   PASE
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#more-papers-on-self-supervised-learning">
   More papers on self-supervised learning
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="methods-for-self-supervised-learning">
<h1>Methods for Self-Supervised Learning<a class="headerlink" href="#methods-for-self-supervised-learning" title="Permalink to this headline">¶</a></h1>
<p>In this section, we will discuss several methods of self-supervised learning. While the current literature in self-supervised learning for vision and speech tasks is growing steadily, extracting useful, robust and perhaps even meaningful representations from music in an unsupervised manner remains an open challenge.</p>
<p>In the next paragraphs, we will shortly describe different approaches to self-supervised learning.</p>
<div class="section" id="contrastive-learning">
<h2>Contrastive Learning<a class="headerlink" href="#contrastive-learning" title="Permalink to this headline">¶</a></h2>
<p>Contrastive learning is a method that describes learning representations by way of modeling similarity from natural variations of data, which is often organised into similar and dissimilar pairs, or by way of organising an encoder in such a way that it keeps a dictionary of positive and negative examples.</p>
<p>It is often presented in the following stages:</p>
<ol class="simple">
<li><p>Encode different “views” from natural variations of a single example.</p></li>
<li><p>Compute a similarity metric on the representations from the encoder(s).</p></li>
<li><p>Evaluate the pre-trained representations with linear regression on the downstream task(s). A linear evaluation protocol is chosen so that the contribution, effectiveness and the versatility of the self-supervised learned representations for the downstream task can be tested.</p></li>
</ol>
</div>
<div class="section" id="contrastive-predictive-coding">
<h2>Contrastive Predictive Coding<a class="headerlink" href="#contrastive-predictive-coding" title="Permalink to this headline">¶</a></h2>
<p>Contrastive predictive coding (CPC) was introduced by Aäron van den Oord et al. (2018) as a universal framework of representation learning. It is an approach to self-supervised learning that leverages the structure of the data, to learn representations that encode an underlying, shared information between different parts of the data. The data can be an image, in which neighbouring <em>patches</em> usually share spatial information locally. Or an audio signal, in which phonemes, or even musical beats, share temporal information locally. Conversely, on a more global level we expect the chorus of a song to repeat in another part of our audio signal. Or, when we look at a picture of a single dog, we expect the shape of a dog’s ear to appear at least twice in the entire image.</p>
<p>In CPC, these related observations are mapped similarly as a representation in a latent space. Their hypothesis is that predictions of related observations are often conditionally dependent on similar, high-level pieces of latent information.</p>
<p>To test their hypothesis, they propose the following:</p>
<ol class="simple">
<li><p>First, complex natural data, such as images and audio, are compressed into a latent embedding space. This makes it easier to model the predictions of related observations.</p></li>
<li><p>A more expressive <em>(read: larger, more powerful)</em> autoregressive model uses the representations in this latent space to make predictions for future observations. These observations are each mapped to their representation.</p></li>
<li><p>The InfoNCE loss, which is inspired by Noise Contrastive Estimation, uses a cross-entropy loss to quantify how well the model can classify these future representations from a set of unrelated, <em>“negative”</em> examples.</p></li>
</ol>
<p><img alt="Contrastive Predictive Coding applied to audio. (From: van den Oord, et al. (2018))" src="../_images/cpc_audio.png" /></p>
<div class="tip admonition">
<p class="admonition-title">Mini-batch composition</p>
<p>In CPC, a mini-batch is made of $N$ examples that are chosen randomly from the full training dataset. Within this batch, a single positive example (also known as the <em>anchor</em> sample) and $N-1$ negative examples are used to compute the cross-entropy loss for classifying the correct positive example. In this way, the internal structure of the data is leveraged to obtain a loss signal that we can backpropagate.</p>
</div>
</div>
<div class="section" id="momentum-contrast-moco">
<h2>Momentum Contrast (MoCO)<a class="headerlink" href="#momentum-contrast-moco" title="Permalink to this headline">¶</a></h2>
<p>Momentum Contrast was proposed by Kaiming He et al. (2019). It demonstrated superior performance on seven vision tasks compared to equivalent networks that were trained in a supervised manner, and was one of the first papers to close the gap between unsupervised and supervised learning approaches on some vision tasks. In MoCO, a dictionary of examples in the data are maintained as a queue. Each example in the mini-batch is encoded, and put in front of the queue, while the last item in the dictionary is subsequently dequeued. The dictionary size is not related to the mini-batch size, and can be chosen arbitrarily. The pretext task used in MoCO is to define a contrastive loss on the query and the keys of the dictionary: a query matches the key when the query is an embedding of a different <em>view</em> of the same datapoint. For example: if the query is an embedding of the bassoon solo in Stravinsky’s “Rite of Spring”, it should match with the key that corresponds to the “Rite of Spring”. The encoded query should be similar to its corresponding key, and dissimilar to other keys in the dictionary.</p>
<p>Training a Momentrum Contrast encoder works by way of using positive and negative pairs of examples in a mini-batch. The positive example pairs are made of queries that correspond to keys of the current mini-batch. Conversely, the negative pairs are queries of the current mini-batch and keys from past mini-batches.</p>
<a class="reference internal image-reference" href="../_images/momentum_contrast.png"><img alt="Momentum Contrast training protocol (From Momentum Contrast for Unsupervised Visual Representation Learning (2019)" class="align-center" src="../_images/momentum_contrast.png" style="width: 400px;" /></a>
<p>The keys are encoded by a <em>“slowly progressing”</em> encoder, because the dictionary’s keys are drawn over multiple mini-batches. This encoder is implemented as a momentum-based moving average. We therefore have two encoders: an encoder for the queries and a momentum-encoder for the keys. The main difference between these two encoders, is that the query encoder is updated by backpropagation, while the momentum encoder is updated by a linear interpolation of the query and the momentum encoder.</p>
<div class="tip admonition">
<p class="admonition-title">Tip</p>
<p>An advantage of Momentum Contrast compared to other contrastive learning approaches, is that the batch size is not related to the number of negative examples candidates in the dictionary lookup. Even for smaller batch sizes, the performance of Momentum Contrast is consistent. This can be especially useful for (raw) audio, for which it is often harder to compute with larger mini-batch sizes due to GPU memory constraints.</p>
</div>
<p><img alt="Momentum Contrast vs. other contrastive loss mechanisms (Source: Momentum Contrast for Unsupervised Visual Representation Learning (2019))" src="../_images/momentum_contrast_compared.png" /></p>
</div>
<div class="section" id="simclr">
<h2>SimCLR<a class="headerlink" href="#simclr" title="Permalink to this headline">¶</a></h2>
<p>SimCLR was introduced by Ting Chen et al. (2020) as a simple contrastive learning approach to learn strong visual representations. It leverages strong image data augmentations, large batch sizes, a single large encoder and a simple contrastive loss to pre-train an encoder that learns effective representations. These representations are used to train very effective linear classifiers in various downstream image classification tasks.</p>
<p>For each image example in the mini-batch, two augmented (but correlated!) views are taken. This is done by way of a series of data augmentations that are applied randomly to each example. This will naturally yield $2N$ datapoints per mini-batch. Each of these augmented views are then embedded using a standard ResNet encoder network. While these representations are used during linear evaluation, during the pre-training stage these representations are projected to a different latent space by a small linear layer on which the contrastive loss is computed.</p>
<p>During pre-training, the network only learns from the contrastive loss: the labels are only used during the linear evaluation phase (see the previous section for more details).</p>
<a class="reference internal image-reference" href="../_images/simclr_contrastive_learning.png"><img alt="../_images/simclr_contrastive_learning.png" class="align-center" src="../_images/simclr_contrastive_learning.png" style="width: 400px;" /></a>
</div>
<div class="section" id="contrastive-losses">
<h2>Contrastive Losses<a class="headerlink" href="#contrastive-losses" title="Permalink to this headline">¶</a></h2>
<a class="reference internal image-reference" href="https://i.imgur.com/2uZeF4U.png"><img alt="Figure 1 from &quot;Improved Baselines with Momentum Contrastive Learning&quot; (Chen et al., 2020)" class="align-center" src="https://i.imgur.com/2uZeF4U.png" style="width: 600px;" /></a>
<p>Many contrastive learning methods use a variant of a contrastive loss function, which was first introduced in Noise Contrastive Estimation (Gutmann et al., 2010) and subsequently the InfoNCE loss from Contrastive Predictive Coding (Van den Oord et al., 2018).</p>
<p>This loss can be minimized using a variety of methods, which mostly differ in how they keep track of the keys of data examples. In the case of SimCLR (Chen et al., 2020), a single batch consists both of “positive” and “negative” pairs, which act as “keys” to the original examples. These are updated end-to-end by back-propagation. To increase the complexity of the contrastive learning task, it requires a large batch size to contain more negative examples. Conversely, for Momentum Contrast the negative examples’ keys are maintained in a queue. It only encodes the queries and the positive keys in a single batch.</p>
<p>$$
\mathcal{L}<em>{q, k^{+},\left{k^{-}\right}}=-\log \frac{\exp \left(q \cdot k^{+} / \tau\right)}{\exp \left(q \cdot k^{+} / \tau\right)+\sum</em>{k^{-}} \exp \left(q \cdot k^{-} / \tau\right)}
$$</p>
</div>
<div class="section" id="pase">
<h2>PASE<a class="headerlink" href="#pase" title="Permalink to this headline">¶</a></h2>
<p>PASE was proposed by Santiago Pascual et al. (2019), which demonstrated that useful representations for speech recognition can be learned by defining multiple pretext tasks that jointly optimize an encoder neural network. The encoder distributes the representations of the input data to multiple, small feed-forward neural networks (called <em>workers</em>) that jointly solve different pretext tasks. Each <em>worker</em> is composed of a single hidden layer, and either solves a regression or binary classification task. These smaller feed-forward layers are chosen because the emphasis on learning the more expressive representations is put on the larger encoder, i.e., the encoder should learn more high-level features that can be used by the <em>worker</em> networks to help solve their tasks. After pre-training the network in a self-supervised manner, the learned representations are evaluated in the task of speaker recognition, emotion recognition and phoneme recognition.</p>
<p><img alt="The PASE architecture (Image source: Learning Problem-agnostic Speech Representations from Multiple Self-supervised Tasks (2019))" src="../_images/pase_architecture.png" /></p>
<p>The improved version of PASE, which was called PASE+, uses a set of audio data augmentations to improve the robustness of the learned representations for the downstream task.</p>
</div>
<div class="section" id="more-papers-on-self-supervised-learning">
<h2>More papers on self-supervised learning<a class="headerlink" href="#more-papers-on-self-supervised-learning" title="Permalink to this headline">¶</a></h2>
<p>The following is a short list of important papers in self-supervised learning, of which a few are discussed more in-depth in this tutorial:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Paper</p></th>
<th class="head"><p>Year</p></th>
<th class="head"><p>Tasks</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://arxiv.org/abs/1807.03748">Representation Learning with Contrastive Predictive Coding</a></p></td>
<td><p>2018</p></td>
<td><p>Speech, images, text, reinforcement learning</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</a></p></td>
<td><p>2014</p></td>
<td><p>Theoretical</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://arxiv.org/abs/1505.05192">Unsupervised Visual Representation Learning by Context Prediction</a></p></td>
<td><p>2015</p></td>
<td><p>Images</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://arxiv.org/abs/1911.05722">Momentum Contrast for Unsupervised Visual Representation Learning</a></p></td>
<td><p>2019</p></td>
<td><p>Images</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://arxiv.org/abs/1904.05862">wav2vec: Unsupervised Pre-training for Speech Recognition</a></p></td>
<td><p>2019</p></td>
<td><p>Speech recognition</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://arxiv.org/abs/1904.03416">Learning Problem-agnostic Speech Representations from Multiple Self-supervised Tasks</a></p></td>
<td><p>2019</p></td>
<td><p>Speech recognition</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://arxiv.org/abs/2006.07733">Bootstrap your own latent: A new approach to self-supervised Learning</a></p></td>
<td><p>2020</p></td>
<td><p>Images</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://arxiv.org/abs/2002.05709">A Simple Framework for Contrastive Learning of Visual Representations</a></p></td>
<td><p>2020</p></td>
<td><p>Images</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://arxiv.org/abs/2010.10915">Contrastive learning of general-purpose audio representations</a></p></td>
<td><p>2020</p></td>
<td><p>Sound classification</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://arxiv.org/abs/2103.09410">Contrastive Learning of Musical Representations</a></p></td>
<td><p>2021</p></td>
<td><p>Music classification</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://arxiv.org/abs/2004.10120">Vector Quantized Contrastive Predictive Coding for Template-based Music Generation</a></p></td>
<td><p>2021</p></td>
<td><p>Music generation</p></td>
</tr>
</tbody>
</table>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./part5_beyond"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="introduction.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Introduction</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="self-supervised-learning.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">PyTorch Tutorial</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By The Jupyter Book community<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>