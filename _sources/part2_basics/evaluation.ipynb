{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n",
    "Evaluation of models is one of the most crucial parts of music classification. No matter how many state-of-the-art models are available, the practical performance of the application can be different depending on which model we choose. Hence, proper evaluation metrics that are fit for purpose are essential in the model selection. In this section, we explore widely used evaluation metrics of music classification. Along with the concepts and definitions of evaluation metrics, their implementation using [scikit-learn](https://scikit-learn.org/stable/index.html) library is provided together.\n",
    "\n",
    "\n",
    "Let's explore different evaluation metrics with an example of a binary classification task. We want to assess a classifier that detects vocals in music. Our dataset has ten songs with vocal (blue) and ten songs without vocal (orange). The green circle is a decision boundary of the model. The model predicts that the items in the green circle are vocal music, and the items at the outside circle are instrumental music.\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src = \"https://i.imgur.com/yrTAyqS.png\" width=500>\n",
    "</p>\n",
    "<p align = \"center\">\n",
    "An example of single-label binary classification\n",
    "</p>\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\n",
    "y_true = np.array([False, False, False, False, False, False, False, False, False ,False, True, True, True, True, True, True, True, True, True, True])\n",
    "y_pred = np.array([False, False, False, False, False, False, False, True, True, True, False, False, True, True, True, True, True, True, True, True])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As shown in the figure below, we can separate the predictions into four categories. \n",
    "\n",
    "<p align = \"center\">\n",
    "<img src = \"https://i.imgur.com/3kcDE2L.png\" width=600>\n",
    "</p>\n",
    "<p align = \"center\">\n",
    "Four categories of predictions\n",
    "</p>\n",
    "\n",
    "- True positives (TP): Correctly predicted vocal music (upper left).\n",
    "- False positives (FP): Predicted as vocal music but they are non-vocal music (upper right).\n",
    "- False negatives (FN): Predicted as non-vocal music but they are vocal music (lower left).\n",
    "- True negatives (TN): Correctly predicted non-vocal music (lower right)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "TP = (y_true & y_pred).sum()\n",
    "FP = (~y_true & y_pred).sum()\n",
    "FN = (y_true & ~y_pred).sum()\n",
    "TN = (~y_true & ~y_pred).sum()\n",
    "print('True Positive: %d' % TP)\n",
    "print('False Positive: %d' % FP)\n",
    "print('False Negative: %d' % FN)\n",
    "print('True Negative: %d' % TN)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True Positive: 8\n",
      "False Positive: 3\n",
      "False Negative: 2\n",
      "True Negative: 7\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Accuracy\n",
    "Accuracy is an intuitive evaluation metric to assess classification models. It measures how many items are correctly predicted. The formula of accuracy is:\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=Accuracy%20%3D%20%5Cfrac%7BTP%20%2B%20TN%7D%7BTP%2BTN%2BFP%2BFN%7D\" width=300>\n",
    "</p>\n",
    "<p align = \"center\">\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "sklearn_accuracy = accuracy_score(y_true, y_pred)\n",
    "print('Accuracy (sklearn): %.4f' % sklearn_accuracy)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.7500\n",
      "Accuracy (sklearn): 0.7500\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Precision\n",
    "Precision measures how many retrieved items are truly relevant. Among 11 retrieved items in the green circle, 8 of them are vocal music, and 3 of them are not. The formula of precision is:\n",
    "<p align = \"center\">\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=Precision%20%3D%20%5Cfrac%7BTP%7D%7BTP%2BFP%7D\" width=210>\n",
    "</p>\n",
    "<p align = \"center\">\n",
    "</p>\n",
    "Precision is also known as positive predictive value."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "precision = TP / (TP + FP)\n",
    "print('Precision: %.4f' % precision)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "sklearn_precision = precision_score(y_true, y_pred)\n",
    "print('Precision (sklearn): %.4f' % sklearn_precision)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision: 0.7273\n",
      "Precision (sklearn): 0.7273\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recall\n",
    "Recall measures how many relevant items are correctly retrieved. Among 10 songs with vocal, 8 of them are correctly predicted as vocal music. The formula of recall is:\n",
    "<p align = \"center\">\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=Recall%20%3D%20%5Cfrac%7BTP%7D%7BTP%2BFN%7D\", width=180>\n",
    "</p>\n",
    "<p align = \"center\">\n",
    "</p>\n",
    "Recall is also known as sensitivity or true positive rate. And the opposite term is specificity or true negative rate: how many rejected items are truly negative, i.e., TN / (FP + TN)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "recall = TP / (TP + FN)\n",
    "print('Recall: %.4f' % recall)\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "sklearn_recall = recall_score(y_true, y_pred)\n",
    "print('Recall (sklearn): %.4f' % sklearn_recall)\n",
    "\n",
    "sensitivity = TP / (TP + FN)\n",
    "specificity = TN / (FP + TN)\n",
    "print('Sensitivity: %.4f' % sensitivity)\n",
    "print('Specificity: %.4f' % specificity)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Recall: 0.8000\n",
      "Recall (sklearn): 0.8000\n",
      "Sensitivity: 0.8000\n",
      "Specificity: 0.7000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{tip}\n",
    "- High precision is directly related to user experience. When retrieved items are truly relevant, users can trust the system.\n",
    "- However, a high precision / low recall system only retrieves a few positive items, which end up with low diversity. A lot of relevant items (False negatives) will be discarded.\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### F-measure\n",
    "F-measure or F-score is an evaluation metric of binary classification. The traditional F-measure (F1-score) is defined as the harmonic mean of precision and recall. The maximum value is 1.0, and the lowest is 0 (either precision or recall is zero).\n",
    "<p align = \"center\">\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=F_1%20%3D%202%5Ccdot%5Cfrac%7Bprecision%20%5Ccdot%20recall%7D%7Bprecision%2Brecall%7D\", width=230>\n",
    "</p>\n",
    "<p align = \"center\">\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "F1 = 2 * precision * recall / (precision + recall)\n",
    "print('F1-score: %.4f' % F1)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "sklearn_F1 = f1_score(y_true, y_pred)\n",
    "print('F1-score (sklearn): %.4f' % sklearn_F1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1-score: 0.7619\n",
      "F1-score (sklearn): 0.7619\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{tip}\n",
    "Depending on system requirements, either precision or recall may be more critical. [Fbeta-measure](https://en.wikipedia.org/wiki/F-score) controls the balance of precision and recall using a coefficient beta.\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### High precision vs high recall?\n",
    "The model outputs the likelihood of the input to have vocal between 0 and 1. Hence, to make a final decision, we need to set a threshold. With a high threshold, the model becomes more strict, which means the green circle becomes smaller. The retrieved results by the model for a given query \"vocal music\" will be reliable. However, the model only retrieves a few songs among the entire vocal tracks (i.e., high precision and low recall). This can be observed from the precision-recall curve below. As the threshold gets closer to 1.0, precision goes higher while recall goes lower.\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src = \"https://i.imgur.com/RzdTvS7.png\" width=500>\n",
    "</p>\n",
    "<p align = \"center\">\n",
    "Threshold-varying precision-recall curve\n",
    "</p>\n",
    "\n",
    "On the other hand, if the threshold gets lower, it results in high recall and low precision, which means the system returns any item to be positive. Like this, appropriate decision making of threshold is crucial."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Area under receiver operating characteristic curve (ROC-AUC)\n",
    "As we checked from the precision-recall curve, the model's performance varies by a decision boundary (threshold). The receiver operating characteristic curve (ROC curve) reflects the model's threshold-varying characteristics. The ROC curve is created by plotting true positive rate (TPR) against false positive rate (FPR), where TPR is also known as sensitivity or recall, and FPR is calculated as (1 - specificity).\n",
    "\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src = \"https://i.imgur.com/zutysxZ.png\" width=400>\n",
    "</p>\n",
    "<p align = \"center\">\n",
    "Precision-recall curve\n",
    "</p>\n",
    "\n",
    "In the figure above, a dotted black line indicates the ROC curve of a random classifier, a blue line indicates a better classifier, and an orange line shows a perfect classifier. As a classifier gets better, the area under the curve (AUC) gets wider. We call this area under the ROC curve as ROC-AUC score."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "y_true = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "y_pred_random = np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n",
    "y_pred_blue = np.array([0.1, 0.3, 0.8, 0.6, 0.1, 0.4, 0.5, 0.1, 0.2, 0.2, 0.4, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.6, 0.8, 0.7])\n",
    "y_pred_orange = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_random = roc_auc_score(y_true, y_pred_random)\n",
    "roc_auc_blue = roc_auc_score(y_true, y_pred_blue)\n",
    "roc_auc_orange = roc_auc_score(y_true, y_pred_orange)\n",
    "print('ROC-AUC (random): %.4f' % roc_auc_random)\n",
    "print('ROC-AUC (blue): %.4f' % roc_auc_blue)\n",
    "print('ROC-AUC (orange): %.4f' % roc_auc_orange)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ROC-AUC (random): 0.5000\n",
      "ROC-AUC (blue): 0.8450\n",
      "ROC-AUC (orange): 1.0000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Area under precision-recall curve (PR-AUC)\n",
    "It is known that ROC-AUC may report overly optimistic results with imbalanced data. Therefore, the area under the precision-recall curve (PR-AUC) is often provided together with ROC-AUC. The precision-recall curve is created by plotting precision against recall at different thresholds. Unlike the ROC-AUC score, which has 0.5 as its lowest value, the lowest bound of PR-AUC differs by data. When a model predicts every item to be positive regardless of threshold, the recall will always be 1.0, and precision will be a ratio of positive items w.r.t. all items. Hence, the lowest value of PR-AUC is the ratio of positive items. \n",
    "\n",
    "<p align = \"center\">\n",
    "<img src = \"https://i.imgur.com/xTsyNPF.png\" width=400>\n",
    "</p>\n",
    "<p align = \"center\">\n",
    "Precision-recall curve\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "y_true = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1])\n",
    "y_pred_random = np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n",
    "y_pred = np.array([0.1, 0.3, 0.8, 0.6, 0.1, 0.4, 0.5, 0.1, 0.2, 0.2, 0.4, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.6, 0.8, 0.7])\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "roc_auc_random = roc_auc_score(y_true, y_pred_random)\n",
    "pr_auc = average_precision_score(y_true, y_pred)\n",
    "pr_auc_random = average_precision_score(y_true, y_pred_random)\n",
    "print('ROC-AUC (random): %.4f' % roc_auc_random)\n",
    "print('PR-AUC (random): %.4f' % pr_auc_random)\n",
    "print('ROC-AUC: %.4f' % roc_auc)\n",
    "print('PR-AUC: %.4f' % pr_auc)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ROC-AUC (random): 0.5000\n",
      "PR-AUC (random): 0.1000\n",
      "ROC-AUC: 0.8472\n",
      "PR-AUC: 0.2917\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{warning}\n",
    "The average precision (`sklearn.metrics.average_precision_score`) is one method for calculating PR-AUC. There are other methods such as trapezoid estimates and the interpolated estimates.\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{tip}\n",
    "When the classification task has multiple labels, we need to aggregate multiple ROC-AUC scores and PR-AUC scores. In scikit-learn library, there is an option called `average`. Most automatic music tagging research uses the option `average='macro'`, which averages tag-wise metrics. For more details, check their documentation ([roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html), [average_precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html)).\n",
    "\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('tutorial': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "interpreter": {
   "hash": "d07f8a6d01a647f8c201632839541ae5d3157a119d69afa1c4342c330b00f4fd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}