
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Architectures &#8212; Music Classification: Beyond Supervised Learning, Towards Real-world Applications</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Audio Data Augmentations" href="data-augmentation.html" />
    <link rel="prev" title="Introduction" href="introduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Music Classification: Beyond Supervised Learning, Towards Real-world Applications</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p class="caption" role="heading">
 <span class="caption-text">
  The Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part1_intro/what-is-music-classification.html">
   What is Music Classification?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part2_basics/input-representations.html">
   Input Representations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part2_basics/dataset.html">
   Datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part2_basics/problem-formulation.html">
   Problem Formulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part2_basics/evaluation.html">
   Evaluation
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Supervised Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Architectures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data-augmentation.html">
   Audio Data Augmentations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tutorial.html">
   PyTorch tutorial
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Semi-supervised Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part4_beyond/introduction.html">
   Beyond Supervision
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part4_beyond/transfer-learning.html">
   Transfer Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part4_beyond/semi-supervised-learning.html">
   Semi-Supervised Learning
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Self-supervised Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part5_beyond/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part5_beyond/methods.html">
   Methods for Self-Supervised Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part5_beyond/self-supervised-learning.html">
   PyTorch Tutorial
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Towards Real-world Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part7_towards/mlops.html">
   MLOps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part7_towards/underexplored-problems.html">
   Under-Explored Problems in Academia
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Conclusion
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part8_conclusion/_intro.html">
   Conclusion
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../references.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/part3_supervised/architectures.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/music-classification/tutorial"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fully-convolutional-networks-fcns-choi2016automatic">
   Fully Convolutional Networks (FCNs),
   <span class="xref cite">
    choi2016automatic
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vgg-ish-short-chunk-cnns-won2020evaluation">
   VGG-ish / Short-chunk CNNs,
   <span class="xref cite">
    won2020evaluation
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#harmonic-cnns-won2019automatic">
   Harmonic CNNs,
   <span class="xref cite">
    won2019automatic
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#musicnn-pons2019musicnn">
   MusiCNN,
   <span class="xref cite">
    pons2019musicnn
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sample-level-cnns-lee2017sample">
   Sample-level CNNs,
   <span class="xref cite">
    lee2017sample
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convolutional-recurrent-neural-networks-crnns-choi2017convolutional">
   Convolutional Recurrent Neural Networks (CRNNs),
   <span class="xref cite">
    choi2017convolutional
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#music-tagging-transformer-won2021semi">
   Music tagging transformer,
   <span class="xref cite">
    won2021semi
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#which-model-should-we-use">
   Which model should we use?
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="architectures">
<h1>Architectures<a class="headerlink" href="#architectures" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>This tutorial mainly covers deep learning approaches for music classification. Before we jump into the details of different deep architectures, let’s check some essential attributes of music classification models.</p>
<p align = "center">
<img src = "https://i.imgur.com/he0L4nX.png" width=650>
</p>
<p align = "center">
Music classification model
</p>
<p>As shown in the figure above, a music classification model can be broken into preprocessing, front end, and back end modules. In the previous section, we have already covered the preprocessing steps where the model extracts different input representations. The front end of the music classification model usually captures local acoustic characteristics such as timbre, pitch, or existence of a particular instrument. Then the back end module summarizes a sequence of the extracted features, which are the output of the front end module. The boundary between the front and back end may be ambiguous, sometimes.</p>
<p align = "center">
<img src = "https://i.imgur.com/LYeb263.png" width=450>
</p>
<p align = "center">
</p>
<p>Another important attribute of music classification is song-level training vs instance-level training. Although our goal is to make song-level predictions, music classification models often use only short audio segments during the training. This is called instance-level training. Instance-level training is justified by our intuition; that humans can predict music tags (e.g., rock music) with just a few-second snippet. As shown in the figure above, when we train a model with an instance level, we end up having more training examples. The task may become more difficult because the model is given a less amount of information. In practice, sometimes this ends up obtaining a more robust music tagging model, probably due to the higher stochasticity. After training an instance-level model, if we need a song-level prediction, the instance-level predictions can be aggregated. Max-pooling, average-pooling, or majority vote is the common operations used for the aggregation.</p>
<p>We summarize important attributes of music classification models as follow:</p>
<style>
td {
  font-size: 12px
}
</style>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Model</p></th>
<th class="text-align:center head"><p>Preprocessing</p></th>
<th class="text-align:center head"><p>Input length</p></th>
<th class="text-align:center head"><p>Front end</p></th>
<th class="text-align:center head"><p>Back end</p></th>
<th class="text-align:center head"><p>Training</p></th>
<th class="text-align:center head"><p>Aggregation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>FCN</p></td>
<td class="text-align:center"><p>STFT</p></td>
<td class="text-align:center"><p>29.1s</p></td>
<td class="text-align:center"><p>2D CNN</p></td>
<td class="text-align:center"><p>.</p></td>
<td class="text-align:center"><p>song-level</p></td>
<td class="text-align:center"><p>.</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>VGG-ish / Short-chunk CNN</p></td>
<td class="text-align:center"><p>STFT</p></td>
<td class="text-align:center"><p>3.96s</p></td>
<td class="text-align:center"><p>2D CNN</p></td>
<td class="text-align:center"><p>Global max pooling</p></td>
<td class="text-align:center"><p>instance-level</p></td>
<td class="text-align:center"><p>Average</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Harmonic CNN</p></td>
<td class="text-align:center"><p>STFT</p></td>
<td class="text-align:center"><p>5s</p></td>
<td class="text-align:center"><p>2D CNN</p></td>
<td class="text-align:center"><p>Global max pooling</p></td>
<td class="text-align:center"><p>instance-level</p></td>
<td class="text-align:center"><p>Average</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>MusiCNN</p></td>
<td class="text-align:center"><p>STFT</p></td>
<td class="text-align:center"><p>3s</p></td>
<td class="text-align:center"><p>2D CNN</p></td>
<td class="text-align:center"><p>1D CNN</p></td>
<td class="text-align:center"><p>instance-level</p></td>
<td class="text-align:center"><p>Average</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Sample-level CNN</p></td>
<td class="text-align:center"><p>.</p></td>
<td class="text-align:center"><p>3s</p></td>
<td class="text-align:center"><p>1D CNN</p></td>
<td class="text-align:center"><p>1D CNN</p></td>
<td class="text-align:center"><p>instance-level</p></td>
<td class="text-align:center"><p>Average</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>CRNN</p></td>
<td class="text-align:center"><p>STFT</p></td>
<td class="text-align:center"><p>29.1s</p></td>
<td class="text-align:center"><p>2D CNN</p></td>
<td class="text-align:center"><p>RNN</p></td>
<td class="text-align:center"><p>song-level</p></td>
<td class="text-align:center"><p>.</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Music tagging transformer</p></td>
<td class="text-align:center"><p>STFT</p></td>
<td class="text-align:center"><p>5s-30s</p></td>
<td class="text-align:center"><p>2D CNN</p></td>
<td class="text-align:center"><p>Transformer</p></td>
<td class="text-align:center"><p>instance-level</p></td>
<td class="text-align:center"><p>Average</p></td>
</tr>
</tbody>
</table>
<!--|FCN|오른쪽정렬|중앙정렬|
|VGG-ish / Short-chunk CNN|오른쪽정렬|중앙정렬|
|Harmonic CNN|오른쪽정렬|중앙정렬|
|MusiCNN|오른쪽정렬|중앙정렬|
|Sample-level CNN|오른쪽정렬|중앙정렬|
|CRNN|오른쪽정렬|중앙정렬|
|Transformer|오른쪽정렬|중앙정렬|
-->
</div>
<div class="section" id="fully-convolutional-networks-fcns-choi2016automatic">
<h2>Fully Convolutional Networks (FCNs), <span id="id1">[<a class="reference internal" href="../references.html#id13">Choi <em>et al.</em>, 2016</a>]</span><a class="headerlink" href="#fully-convolutional-networks-fcns-choi2016automatic" title="Permalink to this headline">¶</a></h2>
<p>Motivated by the huge success of convolutional neural networks (CNN) in computer vision, MIR researchers adopted the successful architectures to solve automatic music tagging problems. The fully convolutional network (FCN) is one of the early deep learning approaches for music tagging, which comprises four convolutional layers. Each layer is followed by batch normalization, rectified linear unit (ReLU) non-linearity, and a max-pooling layer. 3x3 convolutional filters are used to capture spectro-temporal acoustic characteristics of an input melspectrogram.</p>
<p align = "center">
<img src = "https://i.imgur.com/P0E0zU8.png" width=650>
</p>
<p align = "center">
Fully convolutional network
</p>
In the original paper, the input length of FCN was 29.1s, yielding a 96x1366 melspectrogram. Different sizes of strides were used for max-pooling layers ((2, 4), (4, 5), (3, 8), (4, 8)) to increase the size of receptive fields to cover the entire input melspectrogram
</div>
<div class="section" id="vgg-ish-short-chunk-cnns-won2020evaluation">
<h2>VGG-ish / Short-chunk CNNs, <span id="id2">[<a class="reference internal" href="../references.html#id41">Won <em>et al.</em>, 2020</a>]</span><a class="headerlink" href="#vgg-ish-short-chunk-cnns-won2020evaluation" title="Permalink to this headline">¶</a></h2>
<p>The VGG-ish model and Short-chunk CNNs are very similar to FCN except for their inputs. Instead of learning song-level representation, they utilize instance-level (chunk-level) training.</p>
<p>Since their input length is shorter than FCN’s, the VGG-ish model and Short-chunk CNN do not need to increase the size of receptive fields with sparse strides. Instead, Short-chunk CNN, for example, consists of 7 convolutional layers with dense max-pooling (2, 2), which fits a 3.69s audio chunk. When its input becomes longer, the model summarizes the features using global max pooling.</p>
</div>
<div class="section" id="harmonic-cnns-won2019automatic">
<h2>Harmonic CNNs, <span id="id3">[<a class="reference internal" href="../references.html#id40">Won <em>et al.</em>, 2019</a>]</span><a class="headerlink" href="#harmonic-cnns-won2019automatic" title="Permalink to this headline">¶</a></h2>
<p>The convolutional modules of Harmonic CNNs are identical to those of Short-chunk CNNs, but they use slightly different inputs. Harmonic CNNs take advantage of trainable band-pass filters and harmonically stacked time-frequency representation inputs. In contrast with fixed mel filterbanks, trainable filters bring more flexibility to the model. And harmonically stacked representation preserves spectro-temporal locality while keeping the harmonic structures through the channel of the input tensor in the first convolutional layer.</p>
<p align = "center">
<img src = "https://i.imgur.com/WRBoq51.png" width=650>
</p>
<p align = "center">
Harmonic CNN
</p>
If the convolution filter (red square) in the most front representation captures the activation around 200Hz, the filter in the second channel captures the activation around 400Hz, and 600Hz and 800Hz for the third and the fourth channel respectively. Since harmonic structure plays an important role in the perception of timbre, this spectro-temporal-harmonic representation can be helpful in music classification tasks. In the original paper, Harmonic CNN was trained with 5s audio segments, and the instance-level predictions were aggregated using global average pooling.
</div>
<div class="section" id="musicnn-pons2019musicnn">
<h2>MusiCNN, <span id="id4">[<a class="reference internal" href="../references.html#id50">Pons and Serra, 2019</a>]</span><a class="headerlink" href="#musicnn-pons2019musicnn" title="Permalink to this headline">¶</a></h2>
<p>Instead of using 3x3 filters, the authors of MusiCNN proposed to use manually designed filter shapes for music tagging. Let’s first assume that x- and y-axes correspond to time and frequency. Vertically long filters are designed to capture timbral characteristics, while horizontally long filters are designed to capture temporal energy flux that is probably related to rhythmic patterns and tempo.</p>
<p align = "center">
<img src = "https://i.imgur.com/opVUKoE.png" width=650>
</p>
<p align = "center">
MusiCNN
</p>
To enforce the pitch-invariance, a following max-pooling layer aggregates the input with the maximum value across the frequency axis. Finally, the sequence of extracted timbral and temporal features are summarized in 1D convolutional layers. MusiCNN is trained with 3s audio segments, and the instance-level predictions are aggregated using global average pooling.
</div>
<div class="section" id="sample-level-cnns-lee2017sample">
<h2>Sample-level CNNs, <span id="id5">[<a class="reference internal" href="../references.html#id14">Lee <em>et al.</em>, 2017</a>]</span><a class="headerlink" href="#sample-level-cnns-lee2017sample" title="Permalink to this headline">¶</a></h2>
<p>Sample-level CNNs and its variant tackle automatic music tagging in an end-to-end manner by directly using raw audio waveforms as their inputs. In this architecture, 1x2 or 1x3 (1D) convolution filters are used. Each layer consists of 1D convolution, batch normalization, and ReLU non-linearity. Strided convolution is used to increase the size of the receptive field.</p>
<p align = "center">
<img src = "https://i.imgur.com/Q7z4vmv.png" width=650>
</p>
<p align = "center">
Sample-level CNN
</p>
In this approach, preprocessing steps (e.g., short-time Fourier transform) are not required. Instead, the model is designed to be deeper than spectrogram-based ones so that it can model something equivalent to mel filterbanks. The figure below shows the spectra of learned convolution filters sorted by the frequency of the peak magnitude. The center frequency linearly increases in low frequency, but it goes non-linearly steeper in high frequency. This trend is more evident in deeper layers, and this characteristic can be found in mel filterbanks. 
<p align = "center">
<img src = "https://i.imgur.com/YUIl1PH.png" width=650>
</p>
<p align = "center">
Spectra of the filters in the sample-level convolution layers (x-axis: index of the filter, y-axis: frequency).
</p>
In the original paper, sample-level CNN was trained with 3.69s audio segments. The instance-level predictions are then aggregated using global average pooling.
</div>
<div class="section" id="convolutional-recurrent-neural-networks-crnns-choi2017convolutional">
<h2>Convolutional Recurrent Neural Networks (CRNNs), <span id="id6">[<a class="reference internal" href="../references.html#id51">Choi <em>et al.</em>, 2017</a>]</span><a class="headerlink" href="#convolutional-recurrent-neural-networks-crnns-choi2017convolutional" title="Permalink to this headline">¶</a></h2>
<p>Unlike the previously introduced instance-level models, the convolutional recurrent neural networks (CRNNs) are designed to represent music as a long sequence of multiple instances. CRNNs can be described as a combination of CNN and RNN. The CNN front end captures local acoustic characteristics (instance-level), and the RNN back end summarizes the sequence of instance-level features.</p>
<p align = "center">
<img src = "https://i.imgur.com/Am4drg2.png" width=350>
</p>
<p align = "center">
Convolutional recurrent neural network
</p>
In the original paper, four convolutional layers with 3x3 filters were used in the CNN front end, and two-layer RNN with gated recurrent units (GRU) were used in the back end. The model used 29.1s audio inputs, and it was possible because it does not need a feature aggregation step as RNN summarizes the sequence.
</div>
<div class="section" id="music-tagging-transformer-won2021semi">
<h2>Music tagging transformer, <span id="id7">[<a class="reference internal" href="../references.html#id7">Won <em>et al.</em>, 2021</a>]</span><a class="headerlink" href="#music-tagging-transformer-won2021semi" title="Permalink to this headline">¶</a></h2>
<p>The motivation of the convolutional neural network with self-attention (CNNSA) and Music tagging transformer is identical to that of the CRNN model. The front end captures local acoustic characteristics, and the back end summarizes the sequence. In the field of natural language processing, Transformer has shown its suitability in long sequence modeling by using self-attention layers. Both CNNSA and Music tagging transformer use the CNN front end and the Transformer back end. The back end summarizes the instance-level features effectively.</p>
<p align = "center">
<img src = "https://i.imgur.com/DsGDE5U.png" width=450>
</p>
<p align = "center">
Music tagging transformer
</p>
CNNSA uses the CNN front end of MusiCNN while Music tagging transformer uses the CNN front end of Short-chunk ResNet. The input length can be varied from 5s to 30s.
<p align = "center">
<img src = "https://i.imgur.com/kTJ0kpd.png" width=650>
</p>
<p align = "center">
Tag-wise contribution heatmap of transformer
</p>
Besides its performance, an advantage of using a Transformer back end is the interpretability. Since the attention mechanism is implemented with fully-connected layers, the behavior of the model is easily interpretable. The examples above show concatenated mel spectrograms and their contribution to the tag predictions. From the heatmap, we can see the parts of the audio that are relevant to each tag.
</div>
<div class="section" id="which-model-should-we-use">
<h2>Which model should we use?<a class="headerlink" href="#which-model-should-we-use" title="Permalink to this headline">¶</a></h2>
<p>After exploring this many different architectures, the first natural question would be about the <em>best</em> model to use. In previous works, experimental results in three datasets (MagnaTagATune, Million Song Data, MTG-Jamendo) are reported as follows.</p>
<p align = "center">
<img src = "https://i.imgur.com/5rdBGjX.png" width=650>
</p>
<p align = "center">
Comparison of music tagging models
</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Summary:</p>
<ul class="simple">
<li><p>For the best performance, use the Music tagging transformer.</p></li>
<li><p>VGG-ish and Short-chunk CNN are simple but powerful choices.
When your training dataset is small, try with a reduced search space by using MusiCNN or Harmonic CNN.</p></li>
<li><p>Sample-level CNN achieves strong performance with the increase of the size of the dataset. Still, spectrogram-based models are showing state-of-the-art results.</p></li>
</ul>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul class="simple">
<li><p>PyTorch implementation of introduced models are available online [<a class="reference external" href="https://github.com/minzwon/sota-music-tagging-models.git">Github</a>]</p></li>
<li><p>You can try an online demo of pretrained models [<a class="reference external" href="https://replicate.ai/minzwon/sota-music-tagging-models">Replicate.ai</a>]</p></li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./part3_supervised"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="introduction.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Introduction</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="data-augmentation.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Audio Data Augmentations</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By The Jupyter Book community<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>