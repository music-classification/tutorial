
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Architectures &#8212; Music Classification: Beyond Supervised Learning, Towards Real-world Applications</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Audio Data Augmentations" href="data-augmentation.html" />
    <link rel="prev" title="Introduction" href="introduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Music Classification: Beyond Supervised Learning, Towards Real-world Applications</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  The Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part1_intro/what-is-music-classification.html">
   What is Music Classification?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part2_basics/input-representations.html">
   Input Representations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part2_basics/dataset.html">
   Datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part2_basics/problem-formulation.html">
   Problem Formulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part2_basics/evaluation.html">
   Evaluation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Supervised Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Architectures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data-augmentation.html">
   Audio Data Augmentations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tutorial.html">
   PyTorch tutorial
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Semi-supervised Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part4_beyond/introduction.html">
   Beyond Supervision
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part4_beyond/transfer-learning.html">
   Transfer Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part4_beyond/semi-supervised-learning.html">
   Semi-Supervised Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Self-supervised Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part5_beyond/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part5_beyond/methods.html">
   Methods for Self-Supervised Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part5_beyond/self-supervised-learning.html">
   PyTorch Tutorial
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Towards Real-world Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part7_towards/mlops.html">
   MLOps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part7_towards/underexplored-problems.html">
   Under-Explored Problems in Academia
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Conclusion
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part8_conclusion/_intro.html">
   Conclusion
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../references.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/part3_supervised/architectures.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/music-classification/tutorial"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fully-convolutional-network-fcn-choi2016automatic">
   Fully Convolutional Network (FCN),
   <span class="xref cite">
    choi2016automatic
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vgg-ish-short-chunk-cnn-won2020evaluation">
   VGG-ish / Short-chunk CNN,
   <span class="xref cite">
    won2020evaluation
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#harmonic-cnn-won2019automatic">
   Harmonic CNN,
   <span class="xref cite">
    won2019automatic
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#musicnn-pons2019musicnn">
   MusiCNN,
   <span class="xref cite">
    pons2019musicnn
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sample-level-cnn-lee2017sample">
   Sample-level CNN,
   <span class="xref cite">
    lee2017sample
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convolutional-recurrent-neural-network-crnn-choi2017convolutional">
   Convolutional Recurrent Neural Network (CRNN),
   <span class="xref cite">
    choi2017convolutional
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#music-tagging-transformer-won2021semi">
   Music tagging transformer,
   <span class="xref cite">
    won2021semi
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#which-model-should-we-use">
   Which model should we use?
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="architectures">
<h1>Architectures<a class="headerlink" href="#architectures" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>This tutorial mainly covers deep learning approaches for music classification. Before we jump into the details of different deep architectures, let’s check some essential attributes of music classification models.</p>
<p align = "center">
<img src = "https://i.imgur.com/he0L4nX.png" width=650>
</p>
<p align = "center">
Music classification model
</p>
<p>As shown in the figure above, a music classification model can be broken into preprocessing, front end, and back end modules. In the previous section, we have already covered the preprocessing steps where the model extracts different input representations. The front end of the music classification model usually captures local acoustic characteristics such as timbre, pitch, or existence of a particular instrument. Then the back end module summarizes a sequence of the extracted features, which are the output of the front end module. The boundary between the front and back end may be ambiguous, sometimes.</p>
<p align = "center">
<img src = "https://i.imgur.com/LYeb263.png" width=450>
</p>
<p align = "center">
</p>
<p>Another important attribute of music classification is song-level training vs instance-level training. Although our goal is to make song-level predictions, music classification models often use only short audio segments during the training. This is called instance-level training. Instance-level training is justified by our intuition; that humans can predict music tags (e.g., rock music) with just a few-second snippet. As shown in the figure above, when we train a model with an instance level, we end up having more training examples. The task may become more difficult because the model is given a less amount of information. In practice, sometimes this ends up obtaining a more robust music tagging model, probably due to the higher stochasticity. After training an instance-level model, if we need a song-level prediction, the instance-level predictions can be aggregated. Max-pooling, average-pooling, or majority vote is the common operations used for the aggregation.</p>
<p>We summarize important attributes of music classification models as follow:</p>
<style>
td {
  font-size: 12px
}
</style>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Model</p></th>
<th class="text-align:center head"><p>Preprocessing</p></th>
<th class="text-align:center head"><p>Input length</p></th>
<th class="text-align:center head"><p>Front end</p></th>
<th class="text-align:center head"><p>Back end</p></th>
<th class="text-align:center head"><p>Training</p></th>
<th class="text-align:center head"><p>Aggregation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>FCN</p></td>
<td class="text-align:center"><p>STFT</p></td>
<td class="text-align:center"><p>29.1s</p></td>
<td class="text-align:center"><p>2D CNN</p></td>
<td class="text-align:center"><p>.</p></td>
<td class="text-align:center"><p>song-level</p></td>
<td class="text-align:center"><p>.</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>VGG-ish / Short-chunk CNN</p></td>
<td class="text-align:center"><p>STFT</p></td>
<td class="text-align:center"><p>3.96s</p></td>
<td class="text-align:center"><p>2D CNN</p></td>
<td class="text-align:center"><p>Global max pooling</p></td>
<td class="text-align:center"><p>instance-level</p></td>
<td class="text-align:center"><p>Average</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Harmonic CNN</p></td>
<td class="text-align:center"><p>STFT</p></td>
<td class="text-align:center"><p>5s</p></td>
<td class="text-align:center"><p>2D CNN</p></td>
<td class="text-align:center"><p>Global max pooling</p></td>
<td class="text-align:center"><p>instance-level</p></td>
<td class="text-align:center"><p>Average</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>MusiCNN</p></td>
<td class="text-align:center"><p>STFT</p></td>
<td class="text-align:center"><p>3s</p></td>
<td class="text-align:center"><p>2D CNN</p></td>
<td class="text-align:center"><p>1D CNN</p></td>
<td class="text-align:center"><p>instance-level</p></td>
<td class="text-align:center"><p>Average</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Sample-level CNN</p></td>
<td class="text-align:center"><p>.</p></td>
<td class="text-align:center"><p>3s</p></td>
<td class="text-align:center"><p>1D CNN</p></td>
<td class="text-align:center"><p>1D CNN</p></td>
<td class="text-align:center"><p>instance-level</p></td>
<td class="text-align:center"><p>Average</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>CRNN</p></td>
<td class="text-align:center"><p>STFT</p></td>
<td class="text-align:center"><p>29.1s</p></td>
<td class="text-align:center"><p>2D CNN</p></td>
<td class="text-align:center"><p>RNN</p></td>
<td class="text-align:center"><p>song-level</p></td>
<td class="text-align:center"><p>.</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Music tagging transformer</p></td>
<td class="text-align:center"><p>STFT</p></td>
<td class="text-align:center"><p>5s-30s</p></td>
<td class="text-align:center"><p>2D CNN</p></td>
<td class="text-align:center"><p>Transformer</p></td>
<td class="text-align:center"><p>instance-level</p></td>
<td class="text-align:center"><p>Average</p></td>
</tr>
</tbody>
</table>
<!--|FCN|오른쪽정렬|중앙정렬|
|VGG-ish / Short-chunk CNN|오른쪽정렬|중앙정렬|
|Harmonic CNN|오른쪽정렬|중앙정렬|
|MusiCNN|오른쪽정렬|중앙정렬|
|Sample-level CNN|오른쪽정렬|중앙정렬|
|CRNN|오른쪽정렬|중앙정렬|
|Transformer|오른쪽정렬|중앙정렬|
-->
</div>
<div class="section" id="fully-convolutional-network-fcn-choi2016automatic">
<h2>Fully Convolutional Network (FCN), <span id="id1">[<a class="reference internal" href="../references.html#id13">Choi <em>et al.</em>, 2016</a>]</span><a class="headerlink" href="#fully-convolutional-network-fcn-choi2016automatic" title="Permalink to this headline">¶</a></h2>
<p>Motivated by the huge success of convolutional neural networks (CNN) in computer vision, MIR researchers adopted the successful architectures to solve automatic music tagging problems. The fully convolutional network (FCN) is one of the early deep learning approaches for music tagging, which comprises four convolutional layers. Each layer is followed by batch normalization, rectified linear unit (ReLU) non-linearity, and a max-pooling layer. 3x3 convolutional filters are used to capture spectro-temporal acoustic characteristics of an input melspectrogram.</p>
<p align = "center">
<img src = "https://i.imgur.com/P0E0zU8.png" width=650>
</p>
<p align = "center">
Fully convolutional network
</p>
The input length of FCN is 29.1s, yielding a 96x1366 melspectrogram. Different sizes of strides are used for max-pooling layers ((2, 4), (4, 5), (3, 8), (4, 8)) to increase the size of receptive fields to cover the entire input melspectrogram
</div>
<div class="section" id="vgg-ish-short-chunk-cnn-won2020evaluation">
<h2>VGG-ish / Short-chunk CNN, <span id="id2">[<a class="reference internal" href="../references.html#id41">Won <em>et al.</em>, 2020</a>]</span><a class="headerlink" href="#vgg-ish-short-chunk-cnn-won2020evaluation" title="Permalink to this headline">¶</a></h2>
<p>VGG-ish model and Short-chunk CNN are very similar to FCN except for their inputs. Instead of learning song-level representation, they utilize instance-level (chunk-level) training.</p>
<p>Since the input length is shorter than FCN’s inputs, the VGG-ish model and Short-chunk CNN does not need to increase the size of receptive fields with sparse strides. Instead, Short-chunk CNN, for example, consists of 7 convolutional layers with dense max-pooling (2, 2), which fits a 3.69s audio chunk. When its input becomes longer, the model summarizes the features using global max pooling.</p>
</div>
<div class="section" id="harmonic-cnn-won2019automatic">
<h2>Harmonic CNN, <span id="id3">[<a class="reference internal" href="../references.html#id40">Won <em>et al.</em>, 2019</a>]</span><a class="headerlink" href="#harmonic-cnn-won2019automatic" title="Permalink to this headline">¶</a></h2>
<p>Convolutional modules of Harmonic CNN are identical to Short-chunk CNN, but it uses slightly different inputs. Harmonic CNN takes advantage of trainable band-pass filters and harmonically stacked time-frequency representation inputs. In contrast with fixed Mel filterbanks, trainable filters bring more flexibility to the model. And harmonically stacked representation preserves spectro-temporal locality while keeping the harmonic structures through the channel of the input tensor in the first convolutional layer.</p>
<p align = "center">
<img src = "https://i.imgur.com/WRBoq51.png" width=650>
</p>
<p align = "center">
Harmonic CNN
</p>
If the convolution filter (red square) in the most front representation captures the activation around 200Hz, the filter in the second channel captures the activation around 400Hz, and 600Hz and 800Hz for the third and the fourth channel, respectively. Since harmonic structure plays an important role in timbre perception, this spectro-temporal-harmonic representation can be helpful in automatic music tagging. Harmonic CNN is trained with 5s audio segments, and the instance-level predictions are aggregated using global average pooling.
</div>
<div class="section" id="musicnn-pons2019musicnn">
<h2>MusiCNN, <span id="id4">[<a class="reference internal" href="../references.html#id50">Pons and Serra, 2019</a>]</span><a class="headerlink" href="#musicnn-pons2019musicnn" title="Permalink to this headline">¶</a></h2>
<p>Instead of using 3x3 filters, the authors of MusiCNN proposed to use manually designed filter shapes for music tagging. Vertically long filters are designed to capture timbral characteristics that are relevant to instruments, while horizontally long filters are designed to capture temporal energy flux that is related to rhythmic patterns and tempo.</p>
<p align = "center">
<img src = "https://i.imgur.com/opVUKoE.png" width=650>
</p>
<p align = "center">
MusiCNN
</p>
To enforce the pitch-invariancy, the following max-pooling layer pools the maximum value across the frequency axis. Finally, the sequence of extracted timbral and temporal features are summarized in 1D convolutional layers. MusiCNN is trained with 3s audio segments, and the instance-level predictions are aggregated using global average pooling.
</div>
<div class="section" id="sample-level-cnn-lee2017sample">
<h2>Sample-level CNN, <span id="id5">[<a class="reference internal" href="../references.html#id14">Lee <em>et al.</em>, 2017</a>]</span><a class="headerlink" href="#sample-level-cnn-lee2017sample" title="Permalink to this headline">¶</a></h2>
<p>Sample-level CNN and its variant tackle automatic music tagging in an end-to-end manner by directly using raw audio waveform as their inputs. 1x2 or 1x3 1D convolution filters are used to represent music. Each layer consists of 1D convolution, batch normalization, and ReLU non-linearity. Strided convolution is used to increase the size of the receptive field.</p>
<p align = "center">
<img src = "https://i.imgur.com/Q7z4vmv.png" width=650>
</p>
<p align = "center">
Sample-level CNN
</p>
In this approach, preprocessing steps (e.g., short-time Fourier transform) are not required. Instead, the model is deeper than spectrogram-based ones to model the counterpart of Mel filterbanks. The figure below shows the spectrum of learned convolution filters sorted by the frequency of the peak magnitude. The center frequency linearly increases in low frequency, but it goes non-linearly steeper in high frequency. This trend is more evident in deeper layers, and this characteristic can be found in Mel filterbanks. 
<p align = "center">
<img src = "https://i.imgur.com/YUIl1PH.png" width=650>
</p>
<p align = "center">
Spectrum of the filters in the sample-level convolution layers (x-axis: index of the filter, y-axis: frequency).
</p>
Sample-level CNN and its variant are trained with 3.69s audio segments, and the instance-level predictions are aggregated using global average pooling.
</div>
<div class="section" id="convolutional-recurrent-neural-network-crnn-choi2017convolutional">
<h2>Convolutional Recurrent Neural Network (CRNN), <span id="id6">[<a class="reference internal" href="../references.html#id51">Choi <em>et al.</em>, 2017</a>]</span><a class="headerlink" href="#convolutional-recurrent-neural-network-crnn-choi2017convolutional" title="Permalink to this headline">¶</a></h2>
<p>Different from previously introduced instance-level models, the convolutional recurrent neural network (CRNN) is designed to represent music as a long sequence of multiple instances. A CRNN can be described as a combination of CNN and RNN. The CNN front end captures local acoustic characteristics (instance-level), and the RNN back end summarizes the sequence of instance-level features.</p>
<p align = "center">
<img src = "https://i.imgur.com/Am4drg2.png" width=350>
</p>
<p align = "center">
Convolutional recurrent neural network
</p>
Four convolutional layers with 3x3 filters are used in the CNN front end, and two-layer RNN with gated recurrent units (GRU) are used in the back end. CRNN uses 29.1s audio inputs and does not need a feature aggregation step since RNN summarizes the sequence.
</div>
<div class="section" id="music-tagging-transformer-won2021semi">
<h2>Music tagging transformer, <span id="id7">[<a class="reference internal" href="../references.html#id7">Won <em>et al.</em>, 2021</a>]</span><a class="headerlink" href="#music-tagging-transformer-won2021semi" title="Permalink to this headline">¶</a></h2>
<p>The motivation of the convolutional neural network with self-attention (CNNSA) and Music tagging transformer is identical to CRNN’s one. The front end captures local acoustic characteristics, and the back end summarizes the sequence. In the field of natural language processing, Transformer has shown its suitability in long sequence modeling by stacking self-attention layers. Both CNNSA and Music tagging transformer use the CNN front end, and the Transformer back end summarizes the instance-level features.</p>
<p align = "center">
<img src = "https://i.imgur.com/DsGDE5U.png" width=450>
</p>
<p align = "center">
Music tagging transformer
</p>
CNNSA uses the CNN front end of MusiCNN, and Music tagging transformer uses the CNN front end of Short-chunk ResNet. The input length can be varied from 5s to 30s.
<p align = "center">
<img src = "https://i.imgur.com/kTJ0kpd.png" width=650>
</p>
<p align = "center">
Tag-wise contribution heatmap of transformer
</p>
Another advantage of using a Transformer back end is interpretability. Since the attention mechanism is implemented with fully-connected layers, the model's behavior is very interpretable. The examples above show concatenated Mel spectrograms and their contribution to the tag predictions. The heatmap successfully highlights relevant parts of the audio for each tag.
</div>
<div class="section" id="which-model-should-we-use">
<h2>Which model should we use?<a class="headerlink" href="#which-model-should-we-use" title="Permalink to this headline">¶</a></h2>
<p>After exploring different architectures, the first natural question will be: So, what is the best model to choose? In previous work, experimental results in three datasets (MagnaTagATune, Million Song Data, MTG-Jamendo) are reported as follows.</p>
<p align = "center">
<img src = "https://i.imgur.com/5rdBGjX.png" width=650>
</p>
<p align = "center">
Comparison of music tagging models
</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Summary:</p>
<ul class="simple">
<li><p>For the best performance, use the Music tagging transformer.</p></li>
<li><p>VGG-ish and Short-chunk CNN are simple but powerful choices.
When your training dataset is small, reducing the search space by using MusiCNN or Harmonic CNN is beneficial.</p></li>
<li><p>Sample-level CNN becomes more powerful as the size of the dataset grows. But spectrogram-based models are more powerful yet.</p></li>
</ul>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul class="simple">
<li><p>PyTorch implementation of introduced models are available online [<a class="reference external" href="https://github.com/minzwon/sota-music-tagging-models.git">Github</a>]</p></li>
<li><p>You can try an online demo of pretrained models [<a class="reference external" href="https://replicate.ai/minzwon/sota-music-tagging-models">Replicate.ai</a>]</p></li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./part3_supervised"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="introduction.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Introduction</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="data-augmentation.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Audio Data Augmentations</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By The Jupyter Book community<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>